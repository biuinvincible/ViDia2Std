{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgmUjZLU-lEf"
      },
      "source": [
        "# Intrinsic Evaluation: Vietnamese Dialect-to-Standard Translation\n",
        "\n",
        "This notebook evaluates various sequence-to-sequence models for Vietnamese dialect-to-standard translation.\n",
        "\n",
        "## Models Evaluated\n",
        "- `bmd1905/vietnamese-correction-v2`\n",
        "- `vinai/bartpho-syllable-base`\n",
        "- `vinai/bartpho-word-base`\n",
        "- `VietAI/vit5-base`\n",
        "- `facebook/mbart-large-50`\n",
        "\n",
        "## Reproducibility\n",
        "- **Random Seed**: 42 (set for Python, NumPy, PyTorch, and Transformers)\n",
        "- All experiments use the same seed for reproducibility\n",
        "- Model initialization, data shuffling, and training are deterministic\n",
        "\n",
        "## Usage\n",
        "1. Install dependencies (Cell 1)\n",
        "2. Set model name in Cell 2 (default: `VietAI/vit5-base`)\n",
        "3. Run all cells to train and evaluate the model\n",
        "4. Results are saved in `./results` and model in `./best_model`\n",
        "\n",
        "## Metrics\n",
        "- BLEU: Bilingual Evaluation Understudy\n",
        "- ROUGE-L: Longest Common Subsequence\n",
        "- METEOR: Metric for Evaluation of Translation with Explicit ORdering\n",
        "- WER: Word Error Rate\n",
        "- CER: Character Error Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjuOeZ0JvCfV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==4.0.0 evaluate==0.4.6 accelerate==1.11.0 rouge_score==0.1.2 jiwer==4.0.0 editdistance==0.8.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLZIoGa4vCfY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import evaluate\n",
        "import torch\n",
        "from jiwer import wer as calculate_wer\n",
        "import editdistance\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42  # Seed value used for all experiments\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed for reproducibility across all libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Apply seed\n",
        "set_seed(RANDOM_SEED)\n",
        "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "train_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/train.csv\").dropna()\n",
        "valid_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/dev.csv\").dropna()\n",
        "test_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/test.csv\").dropna()\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "model_name = \"facebook/mbart-large-50\" # [bmd1905/vietnamese-correction-v2, vinai/bartpho-syllable-base, vinai/bartpho-word-base, VietAI/vit5-base, facebook/mbart-large-50]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on device: {device}\")\n",
        "\n",
        "max_length = 50\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"dialect\"]\n",
        "    targets = examples[\"standard\"]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenizer.src_lang = \"vi_VN\"\n",
        "    tokenizer.tgt_lang = \"vi_VN\"\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Load metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Replace -100 with the pad token ID before decoding\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Now decode both sequences\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Calculate ROUGE metrics (with proper key handling)\n",
        "    try:\n",
        "        rouge_result = rouge.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels,\n",
        "            use_stemmer=True\n",
        "        )\n",
        "        # Use rougeL since that's what's available\n",
        "        result[\"rouge_l\"] = rouge_result[\"rougeL\"]\n",
        "    except Exception as e:\n",
        "        print(f\"ROUGE calculation error: {e}\")\n",
        "        result[\"rouge_l\"] = float('nan')\n",
        "\n",
        "    # Calculate BLEU (with corrected format)\n",
        "    try:\n",
        "        # The metric automatically wraps references into list of lists if provided as list of strings (see bleu.py)\n",
        "        bleu_result = bleu.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels\n",
        "        )\n",
        "        result[\"bleu\"] = bleu_result[\"bleu\"]\n",
        "    except Exception as e:\n",
        "        print(f\"BLEU calculation error: {e}\")\n",
        "        # Fallback: calculate BLEU manually using NLTK\n",
        "        try:\n",
        "            bleu_scores = []\n",
        "            smoothing_function = SmoothingFunction().method1\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                pred_tokens = pred.split()\n",
        "                label_tokens = label.split()\n",
        "                if len(label_tokens) > 0:\n",
        "                    score = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothing_function)\n",
        "                    bleu_scores.append(score)\n",
        "            result[\"bleu\"] = np.mean(bleu_scores) if bleu_scores else float('nan')\n",
        "        except Exception as e2:\n",
        "            print(f\"Manual BLEU calculation also failed: {e2}\")\n",
        "            result[\"bleu\"] = float('nan')\n",
        "\n",
        "    # Calculate METEOR\n",
        "    try:\n",
        "        meteor_scores = []\n",
        "        for pred, label in zip(decoded_preds, decoded_labels):\n",
        "            # METEOR expects a list of tokens\n",
        "            pred_tokens = pred.split()\n",
        "            label_tokens = label.split()\n",
        "            if len(label_tokens) > 0:  # Avoid empty references\n",
        "                meteor_scores.append(meteor_score([label_tokens], pred_tokens))\n",
        "\n",
        "        result[\"meteor\"] = np.mean(meteor_scores) if meteor_scores else float('nan')\n",
        "    except Exception as e:\n",
        "        print(f\"METEOR calculation error: {e}\")\n",
        "        result[\"meteor\"] = float('nan')\n",
        "\n",
        "    # Calculate WER (Word Error Rate)\n",
        "    try:\n",
        "        valid_pairs = [(ref, pred) for ref, pred in zip(decoded_labels, decoded_preds) if len(ref.strip()) > 0]\n",
        "\n",
        "        if valid_pairs:\n",
        "            # Unzip the valid pairs\n",
        "            valid_refs, valid_preds = zip(*valid_pairs)\n",
        "\n",
        "            # Calculate WER only on valid pairs\n",
        "            wer_scores = [calculate_wer(ref, pred) for ref, pred in zip(valid_refs, valid_preds)]\n",
        "            result[\"wer\"] = np.mean(wer_scores)\n",
        "        else:\n",
        "            result[\"wer\"] = float('nan')\n",
        "    except Exception as e:\n",
        "        print(f\"WER calculation error: {e}\")\n",
        "        result[\"wer\"] = float('nan')\n",
        "\n",
        "    # Calculate CER (Character Error Rate)\n",
        "    def calculate_cer(ref, pred):\n",
        "        if len(ref) == 0:\n",
        "            return 1.0 if len(pred) > 0 else 0.0\n",
        "        return editdistance.eval(ref, pred) / max(len(ref), 1)\n",
        "\n",
        "    try:\n",
        "        cer_scores = [calculate_cer(ref, pred) for ref, pred in zip(decoded_labels, decoded_preds)]\n",
        "        result[\"cer\"] = np.mean(cer_scores)\n",
        "    except Exception as e:\n",
        "        print(f\"CER calculation error: {e}\")\n",
        "        result[\"cer\"] = float('nan')\n",
        "\n",
        "    # Round all results for better readability\n",
        "    return {k: round(v, 4) if not isinstance(v, float) or not np.isnan(v) else v for k, v in result.items()}\n",
        "\n",
        "# Enhanced Training Arguments with Early Stopping Configuration\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"eval_bleu\",  # Primary metric for early stopping decision\n",
        "    greater_is_better=True,  # Higher BLEU scores are better\n",
        "    save_strategy=\"epoch\",  # Save model after each epoch\n",
        "    load_best_model_at_end=True,  # Load the best performing model at the end\n",
        "    eval_accumulation_steps=1,\n",
        "    save_steps=500,  # Additional checkpoint saving\n",
        "    logging_first_step=True,  # Log the first training step\n",
        "    dataloader_pin_memory=torch.cuda.is_available(),  # Performance optimization\n",
        "    seed=RANDOM_SEED,  # Set seed for data shuffling and training\n",
        "    data_seed=RANDOM_SEED,  # Set seed for data sampling\n",
        ")\n",
        "\n",
        "# Initialize Early Stopping Callback with enhanced configuration\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=1,  # Stop if no improvement for 3 consecutive evaluations\n",
        "    early_stopping_threshold=0.01  # Minimum improvement threshold\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Training with early stopping monitoring\n",
        "print(\"Starting training with early stopping...\")\n",
        "print(f\"Maximum epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Early stopping patience: {early_stopping_callback.early_stopping_patience}\")\n",
        "print(f\"Monitoring metric: {training_args.metric_for_best_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6yF6ytGvCfZ"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9roULEpvCfZ"
      },
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(tokenized_test, metric_key_prefix=\"test\")\n",
        "print(f\"Test results: {test_results}\")\n",
        "\n",
        "# Function for inference on new text\n",
        "def normalize_text(text, max_length=50):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate prediction with deterministic settings\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        num_beams=1,  # No beam search for deterministic results\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode prediction\n",
        "    normalized_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return normalized_text\n",
        "\n",
        "model_save_path = \"./best_model\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOusPumgvCfZ"
      },
      "outputs": [],
      "source": [
        "def translate(text, max_length=50):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate with deterministic settings for reproducibility\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        num_beams=1,  # No beam search for deterministic results\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqamwl9evCfa"
      },
      "outputs": [],
      "source": [
        "print(translate(\"Cấy mấn ni nỏ hở rứa mô, hấn có phần lót bên trong là áo da màu nude mà\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQqAJ4CioXi8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8040725,
          "sourceId": 12729816,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
