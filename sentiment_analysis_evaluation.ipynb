{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wif1WB5Qb78"
      },
      "source": [
        "# Extrinsic Evaluation: Sentiment Analysis\n",
        "\n",
        "This notebook evaluates the impact of dialect normalization on sentiment analysis performance.\n",
        "\n",
        "## Overview\n",
        "- Compares sentiment analysis performance on:\n",
        "  - Original dialect texts\n",
        "  - Normalized/translated texts  \n",
        "  - Standard texts (reference)\n",
        "- Uses pre-trained model: `5CD-AI/Vietnamese-Sentiment-visobert`\n",
        "\n",
        "## Reproducibility\n",
        "- **Random Seed**: 42 (set for Python, NumPy, and PyTorch)\n",
        "- All experiments use the same seed for reproducibility\n",
        "\n",
        "## Metrics\n",
        "- Accuracy\n",
        "- Precision (weighted)\n",
        "- Recall (weighted)\n",
        "- F1-score (weighted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_yUoYD9Qb79"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==4.0.0 evaluate==0.4.6 accelerate==1.11.0 rouge_score==0.1.2 jiwer==4.0.0 editdistance==0.8.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJVKblztQb7-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import evaluate\n",
        "import torch\n",
        "from jiwer import wer as calculate_wer\n",
        "import editdistance\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42  # Seed value used for all experiments\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed for reproducibility across all libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Apply seed\n",
        "set_seed(RANDOM_SEED)\n",
        "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "train_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/train.csv\").dropna()\n",
        "valid_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/dev.csv\").dropna()\n",
        "test_df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/test.csv\").dropna()\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "model_name = \"VietAI/vit5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on device: {device}\")\n",
        "\n",
        "\n",
        "max_length = 50\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"dialect\"]\n",
        "    targets = examples[\"standard\"]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenizer.src_lang = \"vi_VN\"\n",
        "    tokenizer.tgt_lang = \"vi_VN\"\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Load metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Replace -100 with the pad token ID before decoding\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Now decode both sequences\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Calculate ROUGE metrics (with proper key handling)\n",
        "    try:\n",
        "        rouge_result = rouge.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels,\n",
        "            use_stemmer=True\n",
        "        )\n",
        "        # Use rougeL since that's what's available\n",
        "        result[\"rouge_l\"] = rouge_result[\"rougeL\"]\n",
        "    except Exception as e:\n",
        "        print(f\"ROUGE calculation error: {e}\")\n",
        "        result[\"rouge_l\"] = float('nan')\n",
        "\n",
        "    # Calculate BLEU (with corrected format)\n",
        "    try:\n",
        "        # The metric automatically wraps references into list of lists if provided as list of strings (see bleu.py)\n",
        "        bleu_result = bleu.compute(\n",
        "            predictions=decoded_preds,\n",
        "            references=decoded_labels\n",
        "        )\n",
        "        result[\"bleu\"] = bleu_result[\"bleu\"]\n",
        "    except Exception as e:\n",
        "        print(f\"BLEU calculation error: {e}\")\n",
        "        # Fallback: calculate BLEU manually using NLTK\n",
        "        try:\n",
        "            bleu_scores = []\n",
        "            smoothing_function = SmoothingFunction().method1\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                pred_tokens = pred.split()\n",
        "                label_tokens = label.split()\n",
        "                if len(label_tokens) > 0:\n",
        "                    score = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothing_function)\n",
        "                    bleu_scores.append(score)\n",
        "            result[\"bleu\"] = np.mean(bleu_scores) if bleu_scores else float('nan')\n",
        "        except Exception as e2:\n",
        "            print(f\"Manual BLEU calculation also failed: {e2}\")\n",
        "            result[\"bleu\"] = float('nan')\n",
        "\n",
        "    # Calculate METEOR\n",
        "    try:\n",
        "        meteor_scores = []\n",
        "        for pred, label in zip(decoded_preds, decoded_labels):\n",
        "            # METEOR expects a list of tokens\n",
        "            pred_tokens = pred.split()\n",
        "            label_tokens = label.split()\n",
        "            if len(label_tokens) > 0:  # Avoid empty references\n",
        "                meteor_scores.append(meteor_score([label_tokens], pred_tokens))\n",
        "\n",
        "        result[\"meteor\"] = np.mean(meteor_scores) if meteor_scores else float('nan')\n",
        "    except Exception as e:\n",
        "        print(f\"METEOR calculation error: {e}\")\n",
        "        result[\"meteor\"] = float('nan')\n",
        "\n",
        "    # Calculate WER (Word Error Rate)\n",
        "    try:\n",
        "        valid_pairs = [(ref, pred) for ref, pred in zip(decoded_labels, decoded_preds) if len(ref.strip()) > 0]\n",
        "\n",
        "        if valid_pairs:\n",
        "            # Unzip the valid pairs\n",
        "            valid_refs, valid_preds = zip(*valid_pairs)\n",
        "\n",
        "            # Calculate WER only on valid pairs\n",
        "            wer_scores = [calculate_wer(ref, pred) for ref, pred in zip(valid_refs, valid_preds)]\n",
        "            result[\"wer\"] = np.mean(wer_scores)\n",
        "        else:\n",
        "            result[\"wer\"] = float('nan')\n",
        "    except Exception as e:\n",
        "        print(f\"WER calculation error: {e}\")\n",
        "        result[\"wer\"] = float('nan')\n",
        "\n",
        "    # Calculate CER (Character Error Rate)\n",
        "    def calculate_cer(ref, pred):\n",
        "        if len(ref) == 0:\n",
        "            return 1.0 if len(pred) > 0 else 0.0\n",
        "        return editdistance.eval(ref, pred) / max(len(ref), 1)\n",
        "\n",
        "    try:\n",
        "        cer_scores = [calculate_cer(ref, pred) for ref, pred in zip(decoded_labels, decoded_preds)]\n",
        "        result[\"cer\"] = np.mean(cer_scores)\n",
        "    except Exception as e:\n",
        "        print(f\"CER calculation error: {e}\")\n",
        "        result[\"cer\"] = float('nan')\n",
        "\n",
        "    # Round all results for better readability\n",
        "    return {k: round(v, 4) if not isinstance(v, float) or not np.isnan(v) else v for k, v in result.items()}\n",
        "\n",
        "# Enhanced Training Arguments with Early Stopping Configuration\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,  # Maximum epochs - early stopping will terminate earlier if needed\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"eval_bleu\",  # Primary metric for early stopping decision\n",
        "    greater_is_better=True,  # Higher BLEU scores are better\n",
        "    save_strategy=\"epoch\",  # Save model after each epoch\n",
        "    load_best_model_at_end=True,  # Load the best performing model at the end\n",
        "    eval_accumulation_steps=1,  # Evaluate immediately without accumulation\n",
        "    save_steps=500,  # Additional checkpoint saving\n",
        "    logging_first_step=True,  # Log the first training step\n",
        "    dataloader_pin_memory=torch.cuda.is_available(),  # Performance optimization\n",
        "    seed=RANDOM_SEED,  # Set seed for data shuffling and training\n",
        "    data_seed=RANDOM_SEED,  # Set seed for data sampling\n",
        ")\n",
        "\n",
        "# Initialize Early Stopping Callback with enhanced configuration\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=1,  # Stop if no improvement for 3 consecutive evaluations\n",
        "    early_stopping_threshold=0.01  # Minimum improvement threshold\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Training with early stopping monitoring\n",
        "print(\"Starting training with early stopping...\")\n",
        "print(f\"Maximum epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Early stopping patience: {early_stopping_callback.early_stopping_patience}\")\n",
        "print(f\"Monitoring metric: {training_args.metric_for_best_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5tuYNJXQb7_"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0alVxwR8Qb7_"
      },
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(tokenized_test, metric_key_prefix=\"test\")\n",
        "print(f\"Test results: {test_results}\")\n",
        "\n",
        "# Function for inference on new text\n",
        "def normalize_text(text, max_length=25):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate prediction with deterministic settings\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        num_beams=1,  # No beam search for deterministic results\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode prediction\n",
        "    normalized_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return normalized_text\n",
        "\n",
        "model_save_path = \"./best_model\"\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPyNpT8OQb8A"
      },
      "outputs": [],
      "source": [
        "def translate(text, max_length=128):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate with deterministic settings for reproducibility\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,  # Use greedy decoding for reproducibility\n",
        "        num_beams=1,  # No beam search for deterministic results\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKwJ3W6wQb8A"
      },
      "outputs": [],
      "source": [
        "print(translate(\"Cấy mấn ni nỏ hở rứa mô, hấn có phần lót bên trong là áo da màu nude mà\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT8G0xUAQb8A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility (same as Cell 2)\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "pipe = pipeline(\"text-classification\",\n",
        "                model=\"5CD-AI/Vietnamese-Sentiment-visobert\",\n",
        "                device=device,\n",
        "                batch_size=64,\n",
        "                truncation=True,\n",
        "                padding=True)\n",
        "\n",
        "def translate_batch(texts, batch_size=32, max_length=128):\n",
        "    \"\"\"Translate a batch of texts at once\"\"\"\n",
        "    if len(texts) == 0:\n",
        "        return []\n",
        "\n",
        "    inputs = tokenizer(texts,\n",
        "                       return_tensors=\"pt\",\n",
        "                       truncation=True,\n",
        "                       padding=\"max_length\",\n",
        "                       max_length=512)\n",
        "\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_length=max_length,\n",
        "                                 num_beams=1,  # No beam search for deterministic results\n",
        "                                 do_sample=False,  # Use greedy decoding for reproducibility\n",
        "                                 pad_token_id=tokenizer.pad_token_id,\n",
        "                                 eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    translated_texts = [tokenizer.decode(output, skip_special_tokens=True)\n",
        "                        for output in outputs]\n",
        "\n",
        "    return translated_texts\n",
        "\n",
        "def translate_all_texts(texts, batch_size=32, max_length=128):\n",
        "    \"\"\"Translate all texts with batch processing\"\"\"\n",
        "    all_translated = []\n",
        "\n",
        "    print(f\"Translating {len(texts)} texts in batches of {batch_size}...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translation\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch, batch_size, max_length)\n",
        "        all_translated.extend(translated_batch)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return all_translated\n",
        "\n",
        "def predict_sentiment_optimized(texts, batch_size=64):\n",
        "    \"\"\"Predict sentiment with larger batch size and optimization\"\"\"\n",
        "    if len(texts) == 0:\n",
        "        return []\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    print(f\"Analyzing sentiment for {len(texts)} texts in batches of {batch_size}...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Sentiment Analysis\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        try:\n",
        "            results = pipe(batch)\n",
        "            predictions.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {i//batch_size}: {e}\")\n",
        "            for text in batch:\n",
        "                try:\n",
        "                    result = pipe(text)\n",
        "                    predictions.extend(result if isinstance(result, list) else [result])\n",
        "                except:\n",
        "                    predictions.append({'label': 'NEU', 'score': 0.0})\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def parallel_sentiment_analysis(text_groups, group_names, max_workers=3):\n",
        "    \"\"\"Perform parallel sentiment analysis for multiple text groups\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    def analyze_group(texts, name):\n",
        "        print(f\"Starting analysis for {name}...\")\n",
        "        predictions = predict_sentiment_optimized(texts)\n",
        "        pred_labels = [convert_sentiment_label(pred['label']) for pred in predictions]\n",
        "        return name, pred_labels\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_name = {\n",
        "            executor.submit(analyze_group, texts, name): name\n",
        "            for texts, name in zip(text_groups, group_names)\n",
        "        }\n",
        "\n",
        "        # Collect results\n",
        "        for future in as_completed(future_to_name):\n",
        "            name, pred_labels = future.result()\n",
        "            results[name] = pred_labels\n",
        "            print(f\"Completed analysis for {name}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def convert_sentiment_label(label):\n",
        "    \"\"\"Convert label from model to standard format\"\"\"\n",
        "    label_map = {\n",
        "        'POS': 'positive',\n",
        "        'NEG': 'negative',\n",
        "        'NEU': 'neutral'\n",
        "    }\n",
        "    return label_map.get(label, label)\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred, label_name):\n",
        "    \"\"\"Calculate metrics\"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\n=== {label_name} ===\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nClassification Report for {label_name}:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title, labels=None):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_improvement_cases(analysis_df):\n",
        "    \"\"\"Analyze improvement cases in detail\"\"\"\n",
        "\n",
        "    # Filter improvement cases (dialect wrong, translated correct)\n",
        "    improved_samples = analysis_df[\n",
        "        (~analysis_df['dialect_correct']) &\n",
        "        (analysis_df['translated_correct'])\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED ANALYSIS OF IMPROVEMENT CASES\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total improvement cases: {len(improved_samples)} out of {len(analysis_df)} samples\")\n",
        "    print(f\"Improvement rate: {len(improved_samples)/len(analysis_df)*100:.2f}%\")\n",
        "\n",
        "    if len(improved_samples) == 0:\n",
        "        print(\"No improvement cases found.\")\n",
        "        return\n",
        "\n",
        "    # Analyze by sentiment categories\n",
        "    print(f\"\\nIMPROVEMENT BY TRUE SENTIMENT:\")\n",
        "    sentiment_improvement = improved_samples['true_sentiment'].value_counts()\n",
        "    for sentiment, count in sentiment_improvement.items():\n",
        "        total_sentiment = len(analysis_df[analysis_df['true_sentiment'] == sentiment])\n",
        "        improvement_rate = count / total_sentiment * 100\n",
        "        print(f\"  {sentiment}: {count}/{total_sentiment} cases ({improvement_rate:.1f}%)\")\n",
        "\n",
        "    # Analyze patterns of incorrect dialect predictions\n",
        "    print(f\"\\nDIALECT PREDICTION ERRORS FIXED BY TRANSLATION:\")\n",
        "    error_patterns = improved_samples.groupby(['true_sentiment', 'dialect_pred']).size()\n",
        "    for (true_sent, pred_sent), count in error_patterns.items():\n",
        "        print(f\"  True: {true_sent} → Dialect predicted: {pred_sent} ({count} cases)\")\n",
        "\n",
        "    # Display detailed examples\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED EXAMPLES OF IMPROVEMENTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Get diverse examples by sentiment\n",
        "    examples_to_show = []\n",
        "    sentiments = improved_samples['true_sentiment'].unique()\n",
        "\n",
        "    for sentiment in sentiments:\n",
        "        sentiment_samples = improved_samples[improved_samples['true_sentiment'] == sentiment]\n",
        "        # Get up to 5 examples for each sentiment\n",
        "        examples_to_show.extend(sentiment_samples.to_dict('records'))\n",
        "\n",
        "    # Limit total number of examples\n",
        "    examples_to_show = examples_to_show  # Up to 15 examples\n",
        "\n",
        "    for i, example in enumerate(examples_to_show, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"EXAMPLE {i}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"True Sentiment: {example['true_sentiment']}\")\n",
        "        print(f\"Original Dialect Text:\")\n",
        "        print(f\"   '{example['dialect']}'\")\n",
        "        print(f\"Translated Text:\")\n",
        "        print(f\"   '{example['translated']}'\")\n",
        "        print(f\"[INCORRECT] Dialect Prediction: {example['dialect_pred']}\")\n",
        "        print(f\"[CORRECT] Translated Prediction: {example['translated_pred']}\")\n",
        "        print(f\"Standard Text Prediction: {example['standard_pred']}\")\n",
        "\n",
        "        # Add analysis about text length\n",
        "        dialect_len = len(example['dialect'].split())\n",
        "        translated_len = len(example['translated'].split())\n",
        "        print(f\"Text Length: Dialect ({dialect_len} words) → Translated ({translated_len} words)\")\n",
        "\n",
        "    # Overall statistics about improvement patterns\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"IMPROVEMENT STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Compare text lengths\n",
        "    improved_samples['dialect_length'] = improved_samples['dialect'].apply(lambda x: len(x.split()))\n",
        "    improved_samples['translated_length'] = improved_samples['translated'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    print(f\"Average text length in improved cases:\")\n",
        "    print(f\"  Dialect: {improved_samples['dialect_length'].mean():.1f} words\")\n",
        "    print(f\"  Translated: {improved_samples['translated_length'].mean():.1f} words\")\n",
        "\n",
        "    # Analyze sentiment distribution in improved cases\n",
        "    print(f\"\\nSentiment distribution in improvement cases:\")\n",
        "    for sentiment in ['POSITIVE', 'NEGATIVE', 'NEUTRAL']:\n",
        "        count = len(improved_samples[improved_samples['true_sentiment'] == sentiment])\n",
        "        percentage = count / len(improved_samples) * 100\n",
        "        print(f\"  {sentiment}: {count} cases ({percentage:.1f}%)\")\n",
        "\n",
        "    return improved_samples\n",
        "\n",
        "def analyze_regression_cases(analysis_df):\n",
        "    \"\"\"Analyze regression cases in detail (cases that became worse after translation)\"\"\"\n",
        "\n",
        "    # Filter regression cases (dialect correct, translated wrong)\n",
        "    regression_samples = analysis_df[\n",
        "        (analysis_df['dialect_correct']) &\n",
        "        (~analysis_df['translated_correct'])\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED ANALYSIS OF REGRESSION CASES\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total regression cases: {len(regression_samples)} out of {len(analysis_df)} samples\")\n",
        "    print(f\"Regression rate: {len(regression_samples)/len(analysis_df)*100:.2f}%\")\n",
        "\n",
        "    if len(regression_samples) == 0:\n",
        "        print(\"No regression cases found.\")\n",
        "        return\n",
        "\n",
        "    # Analyze by sentiment categories\n",
        "    print(f\"\\nREGRESSION BY TRUE SENTIMENT:\")\n",
        "    sentiment_regression = regression_samples['true_sentiment'].value_counts()\n",
        "    for sentiment, count in sentiment_regression.items():\n",
        "        total_sentiment = len(analysis_df[analysis_df['true_sentiment'] == sentiment])\n",
        "        regression_rate = count / total_sentiment * 100\n",
        "        print(f\"  {sentiment}: {count}/{total_sentiment} cases ({regression_rate:.1f}%)\")\n",
        "\n",
        "    # Analyze patterns of incorrect translated predictions\n",
        "    print(f\"\\nTRANSLATED PREDICTION ERRORS (ORIGINALLY CORRECT):\")\n",
        "    error_patterns = regression_samples.groupby(['true_sentiment', 'translated_pred']).size()\n",
        "    for (true_sent, pred_sent), count in error_patterns.items():\n",
        "        print(f\"  True: {true_sent} → Translated predicted: {pred_sent} ({count} cases)\")\n",
        "\n",
        "    # Display detailed examples\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED EXAMPLES OF REGRESSIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Get up to 10 examples\n",
        "    examples_to_show = regression_samples.head(10).to_dict('records')\n",
        "\n",
        "    for i, example in enumerate(examples_to_show, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"REGRESSION EXAMPLE {i}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"True Sentiment: {example['true_sentiment']}\")\n",
        "        print(f\"Original Dialect Text:\")\n",
        "        print(f\"   '{example['dialect']}'\")\n",
        "        print(f\"Translated Text:\")\n",
        "        print(f\"   '{example['translated']}'\")\n",
        "        print(f\"[CORRECT] Dialect Prediction: {example['dialect_pred']}\")\n",
        "        print(f\"[INCORRECT] Translated Prediction: {example['translated_pred']}\")\n",
        "        print(f\"Standard Text Prediction: {example['standard_pred']}\")\n",
        "\n",
        "        # Add analysis about text length\n",
        "        dialect_len = len(example['dialect'].split())\n",
        "        translated_len = len(example['translated'].split())\n",
        "        print(f\"Text Length: Dialect ({dialect_len} words) → Translated ({translated_len} words)\")\n",
        "\n",
        "    return regression_samples\n",
        "\n",
        "def analyze_translated_errors(analysis_df, max_examples=20):\n",
        "    \"\"\"Analyze all prediction error cases after translation\"\"\"\n",
        "\n",
        "    # Filter cases where translated prediction is wrong\n",
        "    translated_errors = analysis_df[~analysis_df['translated_correct']].copy()\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPREHENSIVE ANALYSIS OF TRANSLATED TEXT PREDICTION ERRORS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total error cases: {len(translated_errors)} out of {len(analysis_df)} samples\")\n",
        "    print(f\"Error rate: {len(translated_errors)/len(analysis_df)*100:.2f}%\")\n",
        "\n",
        "    if len(translated_errors) == 0:\n",
        "        print(\"No prediction errors found in translated texts.\")\n",
        "        return\n",
        "\n",
        "    # Classify errors by type\n",
        "    improvement_cases = translated_errors[~translated_errors['dialect_correct']]  # dialect wrong, translated also wrong\n",
        "    regression_cases = translated_errors[translated_errors['dialect_correct']]   # dialect correct, translated wrong\n",
        "\n",
        "    print(f\"\\nERROR BREAKDOWN:\")\n",
        "    print(f\"  Still wrong after translation: {len(improvement_cases)} cases\")\n",
        "    print(f\"  Became wrong after translation: {len(regression_cases)} cases\")\n",
        "\n",
        "    # Statistics of errors by sentiment\n",
        "    print(f\"\\nERRORS BY TRUE SENTIMENT:\")\n",
        "    for sentiment in sorted(translated_errors['true_sentiment'].unique()):\n",
        "        sentiment_errors = translated_errors[translated_errors['true_sentiment'] == sentiment]\n",
        "        total_sentiment = len(analysis_df[analysis_df['true_sentiment'] == sentiment])\n",
        "        error_rate = len(sentiment_errors) / total_sentiment * 100\n",
        "        print(f\"  {sentiment}: {len(sentiment_errors)}/{total_sentiment} errors ({error_rate:.1f}%)\")\n",
        "\n",
        "        # Detailed prediction patterns for this sentiment\n",
        "        prediction_patterns = sentiment_errors['translated_pred'].value_counts()\n",
        "        for pred, count in prediction_patterns.items():\n",
        "            print(f\"    → Predicted as {pred}: {count} cases\")\n",
        "\n",
        "    # Confusion matrix for translated errors\n",
        "    print(f\"\\nCONFUSION PATTERNS IN TRANSLATED ERRORS:\")\n",
        "    error_patterns = translated_errors.groupby(['true_sentiment', 'translated_pred']).size().sort_values(ascending=False)\n",
        "    for (true_sent, pred_sent), count in error_patterns.items():\n",
        "        percentage = count / len(translated_errors) * 100\n",
        "        print(f\"  True: {true_sent} → Predicted: {pred_sent} ({count} cases, {percentage:.1f}%)\")\n",
        "\n",
        "    # Compare with dialect predictions to understand patterns\n",
        "    print(f\"\\nCOMPARISON WITH DIALECT PREDICTIONS:\")\n",
        "    print(\"Cases where both dialect and translated are wrong:\")\n",
        "    both_wrong = translated_errors[~translated_errors['dialect_correct']]\n",
        "    same_wrong_prediction = both_wrong[both_wrong['dialect_pred'] == both_wrong['translated_pred']]\n",
        "    different_wrong_prediction = both_wrong[both_wrong['dialect_pred'] != both_wrong['translated_pred']]\n",
        "\n",
        "    print(f\"  Same wrong prediction: {len(same_wrong_prediction)} cases\")\n",
        "    print(f\"  Different wrong predictions: {len(different_wrong_prediction)} cases\")\n",
        "\n",
        "    # Detailed examples\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED ERROR EXAMPLES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get diverse examples\n",
        "    examples_to_show = []\n",
        "\n",
        "    # Get examples from each sentiment\n",
        "    for sentiment in sorted(translated_errors['true_sentiment'].unique()):\n",
        "        sentiment_errors = translated_errors[translated_errors['true_sentiment'] == sentiment]\n",
        "        examples_to_show.extend(sentiment_errors.head(max_examples//3).to_dict('records'))\n",
        "\n",
        "    # Limit total number of examples\n",
        "    examples_to_show = examples_to_show[:max_examples]\n",
        "\n",
        "    for i, example in enumerate(examples_to_show, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ERROR EXAMPLE {i}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"True Sentiment: {example['true_sentiment']}\")\n",
        "        print(f\"Original Dialect Text:\")\n",
        "        print(f\"   '{example['dialect']}'\")\n",
        "        print(f\"Translated Text:\")\n",
        "        print(f\"   '{example['translated']}'\")\n",
        "        print(f\"Standard Text:\")\n",
        "        print(f\"   '{example['standard']}'\")\n",
        "\n",
        "        # Predictions comparison\n",
        "        dialect_status = \"[CORRECT]\" if example['dialect_correct'] else \"[INCORRECT]\"\n",
        "        translated_status = \"[INCORRECT]\"  # Always wrong because these are error cases\n",
        "        standard_status = \"[CORRECT]\" if example['standard_pred'] == example['true_sentiment'] else \"[INCORRECT]\"\n",
        "\n",
        "        print(f\"Predictions:\")\n",
        "        print(f\"   Dialect: {example['dialect_pred']} {dialect_status}\")\n",
        "        print(f\"   Translated: {example['translated_pred']} {translated_status}\")\n",
        "        print(f\"   Standard: {example['standard_pred']} {standard_status}\")\n",
        "\n",
        "        # Text characteristics\n",
        "        dialect_len = len(example['dialect'].split()) if example['dialect'] else 0\n",
        "        translated_len = len(example['translated'].split()) if example['translated'] else 0\n",
        "        standard_len = len(example['standard'].split()) if example['standard'] else 0\n",
        "\n",
        "        print(f\"Text Lengths:\")\n",
        "        print(f\"   Dialect: {dialect_len} words\")\n",
        "        print(f\"   Translated: {translated_len} words\")\n",
        "        print(f\"   Standard: {standard_len} words\")\n",
        "\n",
        "        # Error type classification\n",
        "        if example['dialect_correct']:\n",
        "            error_type = \"REGRESSION (was correct, became wrong)\"\n",
        "        else:\n",
        "            if example['dialect_pred'] == example['translated_pred']:\n",
        "                error_type = \"PERSISTENT ERROR (same wrong prediction)\"\n",
        "            else:\n",
        "                error_type = \"DIFFERENT ERROR (different wrong prediction)\"\n",
        "\n",
        "        print(f\"Error Type: {error_type}\")\n",
        "\n",
        "    # Final statistics\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"ERROR STATISTICS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Text length analysis\n",
        "    translated_errors['dialect_length'] = translated_errors['dialect'].apply(lambda x: len(str(x).split()))\n",
        "    translated_errors['translated_length'] = translated_errors['translated'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    print(f\"Average text length in error cases:\")\n",
        "    print(f\"  Dialect: {translated_errors['dialect_length'].mean():.1f} words\")\n",
        "    print(f\"  Translated: {translated_errors['translated_length'].mean():.1f} words\")\n",
        "\n",
        "    # Most problematic sentiments\n",
        "    error_rates_by_sentiment = {}\n",
        "    for sentiment in analysis_df['true_sentiment'].unique():\n",
        "        total = len(analysis_df[analysis_df['true_sentiment'] == sentiment])\n",
        "        errors = len(translated_errors[translated_errors['true_sentiment'] == sentiment])\n",
        "        error_rates_by_sentiment[sentiment] = errors / total * 100\n",
        "\n",
        "    print(f\"\\nError rates by sentiment:\")\n",
        "    for sentiment, rate in sorted(error_rates_by_sentiment.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {sentiment}: {rate:.1f}%\")\n",
        "\n",
        "    return translated_errors\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Set seed again in main function for reproducibility\n",
        "    random.seed(RANDOM_SEED)\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "    # Read CSV file\n",
        "    print(\"Reading CSV file...\")\n",
        "    df = pd.read_csv(\"https://huggingface.co/datasets/Biu3010/ViDia2Std/resolve/main/test.csv\").dropna()\n",
        "\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "    print(f\"Sentiment distribution:\")\n",
        "    print(df['sentiment'].value_counts())\n",
        "\n",
        "    # Get data\n",
        "    dialect_texts = df['dialect'].tolist()\n",
        "    standard_texts = df['standard'].tolist()\n",
        "    true_labels = df['sentiment'].tolist()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMING SENTIMENT ANALYSIS WITH BATCH OPTIMIZATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. Translate dialect texts with batch processing\n",
        "    print(\"\\n1. Translating dialect texts to standard...\")\n",
        "    translation_start = time.time()\n",
        "    translated_texts = translate_all_texts(dialect_texts, batch_size=32)\n",
        "    translation_time = time.time() - translation_start\n",
        "    print(f\"Translation completed in {translation_time:.2f} seconds\")\n",
        "\n",
        "    # 2. Parallel sentiment analysis for 3 text groups\n",
        "    print(\"\\n2. Performing parallel sentiment analysis...\")\n",
        "    sentiment_start = time.time()\n",
        "\n",
        "    text_groups = [dialect_texts, translated_texts, standard_texts]\n",
        "    group_names = ['dialect', 'translated', 'standard']\n",
        "\n",
        "    # Run parallel sentiment analysis\n",
        "    sentiment_results = parallel_sentiment_analysis(text_groups, group_names)\n",
        "\n",
        "    dialect_pred_labels = sentiment_results['dialect']\n",
        "    translated_pred_labels = sentiment_results['translated']\n",
        "    standard_pred_labels = sentiment_results['standard']\n",
        "\n",
        "    sentiment_time = time.time() - sentiment_start\n",
        "    print(f\"Sentiment analysis completed in {sentiment_time:.2f} seconds\")\n",
        "\n",
        "    # Create detailed analysis DataFrame immediately after getting results for shared use\n",
        "    analysis_df = pd.DataFrame({\n",
        "        'dialect': dialect_texts,\n",
        "        'standard': standard_texts,\n",
        "        'translated': translated_texts,\n",
        "        'true_sentiment': true_labels,\n",
        "        'dialect_pred': dialect_pred_labels,\n",
        "        'translated_pred': translated_pred_labels,\n",
        "        'standard_pred': standard_pred_labels\n",
        "    })\n",
        "    analysis_df['dialect_correct'] = (analysis_df['true_sentiment'] == analysis_df['dialect_pred'])\n",
        "    analysis_df['translated_correct'] = (analysis_df['true_sentiment'] == analysis_df['translated_pred'])\n",
        "    analysis_df['improvement'] = analysis_df['translated_correct'] & ~analysis_df['dialect_correct']\n",
        "\n",
        "    # 3. Evaluate results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Compare with true labels\n",
        "    metrics_dialect = evaluate_predictions(true_labels, dialect_pred_labels, \"Dialect Texts vs True Labels\")\n",
        "    metrics_translated = evaluate_predictions(true_labels, translated_pred_labels, \"Translated Texts vs True Labels\")\n",
        "    metrics_standard = evaluate_predictions(true_labels, standard_pred_labels, \"Standard Texts vs True Labels\")\n",
        "\n",
        "    # Direct comparison between dialect and translated\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"DIRECT COMPARISON\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    # Agreement between dialect and translated predictions\n",
        "    agreement = sum(1 for a, b in zip(dialect_pred_labels, translated_pred_labels) if a == b)\n",
        "    agreement_rate = agreement / len(dialect_pred_labels)\n",
        "    print(f\"\\nAgreement between dialect and translated predictions: {agreement_rate:.4f} ({agreement}/{len(dialect_pred_labels)})\")\n",
        "\n",
        "    # Create summary table\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Method': ['Dialect Texts', 'Translated Texts', 'Standard Texts'],\n",
        "        'Accuracy': [metrics_dialect['accuracy'], metrics_translated['accuracy'], metrics_standard['accuracy']],\n",
        "        'Precision': [metrics_dialect['precision'], metrics_translated['precision'], metrics_standard['precision']],\n",
        "        'Recall': [metrics_dialect['recall'], metrics_translated['recall'], metrics_standard['recall']],\n",
        "        'F1-Score': [metrics_dialect['f1'], metrics_translated['f1'], metrics_standard['f1']]\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY TABLE\")\n",
        "    print(\"=\"*50)\n",
        "    print(summary_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "    # *** NEW STATISTICS TABLE ***\n",
        "    # Add statistics table for correct/incorrect sample counts\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PREDICTION CORRECT/INCORRECT COUNT STATISTICS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    dialect_correct_count = analysis_df['dialect_correct'].sum()\n",
        "    dialect_incorrect_count = len(df) - dialect_correct_count\n",
        "\n",
        "    translated_correct_count = analysis_df['translated_correct'].sum()\n",
        "    translated_incorrect_count = len(df) - translated_correct_count\n",
        "\n",
        "    stats_data = {\n",
        "        'Status': ['Correct Predictions', 'Incorrect Predictions', 'Total'],\n",
        "        'Before Normalization (Dialect)': [dialect_correct_count, dialect_incorrect_count, len(df)],\n",
        "        'After Normalization (Translated)': [translated_correct_count, translated_incorrect_count, len(df)]\n",
        "    }\n",
        "    stats_df = pd.DataFrame(stats_data)\n",
        "\n",
        "    print(stats_df.to_string(index=False))\n",
        "    # *** END OF NEW TABLE ***\n",
        "\n",
        "    # Performance metrics\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\" + \"=\"*30)\n",
        "    print(\"PERFORMANCE METRICS\")\n",
        "    print(\"=\"*30)\n",
        "    print(f\"Total execution time: {total_time:.2f} seconds\")\n",
        "    print(f\"Translation time: {translation_time:.2f} seconds\")\n",
        "    print(f\"Sentiment analysis time: {sentiment_time:.2f} seconds\")\n",
        "    print(f\"Average time per sample: {total_time/len(df):.4f} seconds\")\n",
        "\n",
        "    # Improvement calculation\n",
        "    acc_improvement = metrics_translated['accuracy'] - metrics_dialect['accuracy']\n",
        "    f1_improvement = metrics_translated['f1'] - metrics_dialect['f1']\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*30)\n",
        "    print(\"IMPROVEMENT ANALYSIS\")\n",
        "    print(\"=\"*30)\n",
        "    print(f\"Accuracy improvement: {acc_improvement:+.4f}\")\n",
        "    print(f\"F1-score improvement: {f1_improvement:+.4f}\")\n",
        "\n",
        "    if acc_improvement > 0:\n",
        "        print(\"Translation improves sentiment analysis accuracy.\")\n",
        "    else:\n",
        "        print(\"Translation does not improve sentiment analysis accuracy.\")\n",
        "\n",
        "    # DETAILED ANALYSIS OF IMPROVEMENT CASES\n",
        "    improved_cases = analyze_improvement_cases(analysis_df)\n",
        "\n",
        "    # DETAILED ANALYSIS OF REGRESSION CASES (CASES THAT BECAME WORSE)\n",
        "    regression_cases = analyze_regression_cases(analysis_df)\n",
        "\n",
        "    # COMPREHENSIVE ANALYSIS OF ALL PREDICTION ERROR CASES AFTER TRANSLATION\n",
        "    translated_error_cases = analyze_translated_errors(analysis_df, max_examples=15)\n",
        "\n",
        "    # Save results\n",
        "    analysis_df.to_csv('sentiment_analysis_results.csv', index=False, encoding='utf-8')\n",
        "    summary_df.to_csv('sentiment_metrics_summary.csv', index=False)\n",
        "\n",
        "    # Save improvement cases separately\n",
        "    if len(improved_cases) > 0:\n",
        "        improved_cases.to_csv('improvement_cases_detailed.csv', index=False, encoding='utf-8')\n",
        "        print(f\"\\nImprovement cases saved to 'improvement_cases_detailed.csv'\")\n",
        "\n",
        "    # Save regression cases separately\n",
        "    if len(regression_cases) > 0:\n",
        "        regression_cases.to_csv('regression_cases_detailed.csv', index=False, encoding='utf-8')\n",
        "        print(f\"Regression cases saved to 'regression_cases_detailed.csv'\")\n",
        "\n",
        "    # Save all translation error cases separately\n",
        "    if len(translated_error_cases) > 0:\n",
        "        translated_error_cases.to_csv('translated_error_cases_detailed.csv', index=False, encoding='utf-8')\n",
        "        print(f\"All translated error cases saved to 'translated_error_cases_detailed.csv'\")\n",
        "\n",
        "    print(f\"\\nDetailed results saved to 'sentiment_analysis_results.csv'\")\n",
        "    print(f\"Summary metrics saved to 'sentiment_metrics_summary.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6wL-hfCQb8B"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTZSIXSLQb8B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8040725,
          "sourceId": 12729816,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
